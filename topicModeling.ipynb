{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# make dataframes from txt data\n",
    "# each txt contains one tagesschau transcription\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "import warnings\n",
    "import time\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import de_core_news_lg\n",
    "nlp = de_core_news_lg.load(disable=['parser','ner'])\n",
    "#nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "import datetime\n",
    "from joblib import Parallel, delayed\n",
    "from functools import partial\n",
    "from gensim import corpora,models\n",
    "import re\n",
    "\n",
    "# load the s2t transcriptions\n",
    "def load_transcriptions(path):\n",
    "    os.chdir(path)\n",
    "    transcriptions = os.listdir()#[:30]\n",
    "    return transcriptions\n",
    "\n",
    "# preprocessing for topic modelling, in this case LDA (latent dirichlet allocation)\n",
    "def lemmatize_pipe(doc):\n",
    "    lemma_list = [tok.lemma_ for tok in doc]# if str(tok).isalpha] #usually str(tok.lemma_).lower()\n",
    "    return lemma_list\n",
    "\n",
    "def preprocess_pipe(texts,nouns_array):#,non_nouns_array):\n",
    "    preproc_pipe = []\n",
    "    #non_nouns = non_nouns_array\n",
    "    nounss = nouns_array\n",
    "    for doc in nlp.pipe(texts, batch_size=int(len(transcriptions)/n_cpu_cores),n_process=int(n_cpu_cores)):\n",
    "        preproc_pipe.append(lemmatize_pipe(doc))\n",
    "        #non_nouns.extend([word for word in doc if word.pos_!='NOUN'])\n",
    "        #nounss.append([str(chunk.text) for chunk in doc.noun_chunks])\n",
    "        nounss.append([str(word) for word in doc if not word.is_stop and word.pos_=='NOUN'])# or word.pos_=='VERB'])# or word.pos_=='ADJ'])\n",
    "    return preproc_pipe,nounss#,non_nouns\n",
    "\n",
    "def preprocess_transcriptions(transcripts_df,number_cpu_cores=os.cpu_count()-1,manual_stopwords=[]):\n",
    "    # init array for all nnouns found by spacy\n",
    "    nounss = []\n",
    "    #print(\"Use: \"+str(len(transcriptions))+\" transcriptions, preprocess them (and perform LDA (Latent Dirichlet Allocation))\")\n",
    "    # use following line instead of the line after the folowing to use \"tagesschau\" only\n",
    "    #df = pd.DataFrame(index=[i for i in range(len(transcriptions)) if \"tagessschau\" in transcriptions[i]], columns=['transcriptionName','content','year','month','day'])\n",
    "    df = pd.DataFrame(index=[i for i in range(len(transcriptions))], columns=['transcriptionName','content','preprocessed','year','month','day'])\n",
    "    for transcription in range(len(transcriptions)):\n",
    "        # use following line to use \"tagesschau\" only\n",
    "        #if \"tagesschau\" in str(transcriptions[transcription]):\n",
    "        if transcription>-1: #placeholder for above line\n",
    "            with open(transcriptions[transcription], \"r\") as f:\n",
    "                df.loc[transcription,'transcriptionName'] = str(transcriptions[transcription])\n",
    "                df.loc[transcription,'content'] = f.read()\n",
    "    df = df.reset_index()\n",
    "    df1=df.copy()\n",
    "    for transcript in range(len(df)):\n",
    "        # remove \"newline\" and punctuation\n",
    "        #print(df.loc[transcript,'content'])\n",
    "        df.loc[transcript,'content'] = df.loc[transcript,'content'].replace(\"\\n\",\"\").replace(\".\",\"\")\n",
    "\n",
    "    sta = datetime.datetime.now()\n",
    "    df['content'],nounss = preprocess_pipe(df['content'],nounss)#,non_nouns)\n",
    "    print(\"lemmatizing+filtering out nouns needs:\",(datetime.datetime.now()-sta).total_seconds(),\"seconds\")\n",
    "\n",
    "    for i in range(len(nounss)):\n",
    "        nounss[i] = [noun.lower() for noun in nounss[i]]\n",
    "\n",
    "    manual = [[['wetter'],['sonne','regen','wind','schnee','schauer','luft','wolken','gewitter','gewittern']],\n",
    "                 [['himmelsrichtung'],['norden','süden','osten','westen']]]\n",
    "    start_manual=datetime.datetime.now()\n",
    "    for i in range(len(nounss)):\n",
    "        for j in range(len(nounss[i])):\n",
    "            for k in manual:\n",
    "                for l in range(len(k[1])):\n",
    "                    if k[1][l]==nounss[i][j]:\n",
    "                        nounss[i][j] = k[0][0]\n",
    "    end_manual=datetime.datetime.now()\n",
    "    print(\"replacing e.g. 'sonne','regen'... by 'wetter' needs:\",\n",
    "          (end_manual-start_manual).total_seconds(),\"seconds\")    \n",
    "        \n",
    "    manualStopWords = manual_stopwords\n",
    "    removeWords = []\n",
    "    for i in range(len(nounss)):\n",
    "        for j in range(len(nounss[i])):\n",
    "            for k in manualStopWords:\n",
    "                if k in nounss[i][j]:\n",
    "                    #print(\"nounss in manualS\",nounss[i])\n",
    "                    removeWords.append(nounss[i][j])\n",
    "    removeWords = list(dict.fromkeys(removeWords))\n",
    "\n",
    "    for i in range(len(nounss)):\n",
    "        nounss[i] = [word for word in nounss[i] if word not in removeWords]\n",
    "        \n",
    "    for i in range(len(nounss)):\n",
    "        nounss[i] = (','.join(nounss[i])).replace(',',\" \")\n",
    "    \n",
    "    #merge all docs into dataframe\n",
    "    for i in range(len(df)):\n",
    "        df.at[i,'preprocessed'] = nounss[i]\n",
    "    return df,df1\n",
    "\n",
    "def print_workcloud(processed_transcripts):\n",
    "    \n",
    "    # used from tutorial to create a workcloud\n",
    "    all_documents_as_one_string = ','.join(list(processed_transcripts.values))\n",
    "    print(len(all_documents_as_one_string))\n",
    "    #all_documents_as_one_string = ','.join(list(df['content'].values))\n",
    "    wordcloud = WordCloud(width=800,height=400,background_color=\"black\", max_words=500, contour_width=3, contour_color='steelblue')\n",
    "    wordcloud.generate(all_documents_as_one_string)\n",
    "    time.sleep(0.3)\n",
    "    display(wordcloud.to_image())\n",
    "\n",
    "def plot_25_most_common_words(count_data, count_vectorizer):\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    total_counts = np.zeros(len(words))\n",
    "    for t in count_data:\n",
    "        total_counts+=t.toarray()[0]\n",
    "    \n",
    "    count_dict = (zip(words, total_counts))\n",
    "    count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:25]\n",
    "    #print((count_dict))\n",
    "    words = [w[0] for w in count_dict]\n",
    "    counts = [w[1] for w in count_dict]\n",
    "    x_pos = np.arange(len(words)) \n",
    "    #print(x_pos)\n",
    "    plt.figure(2, figsize=(15, 15/1.6180))\n",
    "    plt.subplot(title='25 most common words')\n",
    "    sns.set_context(\"notebook\", font_scale=1.25, rc={\"lines.linewidth\": 2.5})\n",
    "    dfPlot = pd.DataFrame(data={'words':words,'counts':counts})\n",
    "    #display(dfPlot)\n",
    "    sns.barplot(x=words,y=counts,data=dfPlot,palette='husl')\n",
    "    plt.xticks(x_pos, words, rotation=90) \n",
    "    plt.xlabel('words')\n",
    "    plt.ylabel('counts')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def print_topics(model, count_vectorizer, n_top_words):\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % (int(topic_idx)+1),\" \".join([words[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    \n",
    "\n",
    "def train_lda(df,count_vectorizer,count_data,number_topics=10,number_words=3,ngram_tuple=(1,1),stop_words=None):#stopwords.words('german'))\n",
    "    # n_jobs=-1 to use all processors, max_iter=50 randomly chosen, not sure about it\n",
    "    lda = LDA(n_components=n_topics, n_jobs=-1)\n",
    "    lda.fit(count_data)\n",
    "    print(\"LDA Topics:\")\n",
    "    print_topics(lda, count_vectorizer, n_words)\n",
    "    return lda\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number cpu_cores to use: 13\n",
      "There are  2566 transcriptions in given daterange\n",
      "\n",
      "preprocess transcriptions of year: 2007 quantity: 263\n",
      "lemmatizing+filtering out nouns needs: 12.630225 seconds\n",
      "replacing e.g. 'sonne','regen'... by 'wetter' needs: 0.134651 seconds\n",
      "LDA Topics:\n",
      "Topic #1: polizei mitte koalition opposition soldaten\n",
      "Topic #2: gipfel polizei union doping entscheidung\n",
      "Topic #3: partei soldaten union eu mitte\n",
      "Topic #4: soldaten partei bahn einsatz arbeit\n",
      "Topic #5: kritik us partei länder bahn\n",
      "Topic #6: bahn mindestlohn union koalition kinder\n",
      "Topic #7: koalition union mindestlohn soldaten thema\n",
      "Topic #8: bahn gipfel union streik kanzlerin\n",
      "Topic #9: kinder bahn polizei politik damen\n",
      "Topic #10: kinder soldaten richtung familien nordosten\n",
      "Topic #1: polizei mitte koalition opposition soldaten\n",
      "Topic #2: gipfel polizei union doping entscheidung\n",
      "Topic #3: partei soldaten union eu mitte\n",
      "Topic #4: soldaten partei bahn einsatz arbeit\n",
      "Topic #5: kritik us partei länder bahn\n",
      "Topic #6: bahn mindestlohn union koalition kinder\n",
      "Topic #7: koalition union mindestlohn soldaten thema\n",
      "Topic #8: bahn gipfel union streik kanzlerin\n",
      "Topic #9: kinder bahn polizei politik damen\n",
      "Topic #10: kinder soldaten richtung familien nordosten\n",
      "\n",
      "preprocess transcriptions of year: 2008 quantity: 358\n",
      "lemmatizing+filtering out nouns needs: 19.039079 seconds\n",
      "replacing e.g. 'sonne','regen'... by 'wetter' needs: 0.184293 seconds\n",
      "LDA Topics:\n",
      "Topic #1: polizei partei demonstranten sonntag behörden\n",
      "Topic #2: union kinder welt soldaten länder\n",
      "Topic #3: spiele boykott union polizei milch\n",
      "Topic #4: partei banken finanzkrise koalition union\n",
      "Topic #5: richtung chef polizei bank union\n",
      "Topic #6: welt union kanzlerin eu spiel\n",
      "Topic #7: maschine kinder bahn us ice\n",
      "Topic #8: spd union welt spiel soldaten\n",
      "Topic #9: linkspartei union länder entscheidung koalition\n",
      "Topic #10: soldaten stadt polizei lage damen\n",
      "Topic #1: polizei partei demonstranten sonntag behörden\n",
      "Topic #2: union kinder welt soldaten länder\n",
      "Topic #3: spiele boykott union polizei milch\n",
      "Topic #4: partei banken finanzkrise koalition union\n",
      "Topic #5: richtung chef polizei bank union\n",
      "Topic #6: welt union kanzlerin eu spiel\n",
      "Topic #7: maschine kinder bahn us ice\n",
      "Topic #8: spd union welt spiel soldaten\n",
      "Topic #9: linkspartei union länder entscheidung koalition\n",
      "Topic #10: soldaten stadt polizei lage damen\n",
      "\n",
      "preprocess transcriptions of year: 2009 quantity: 355\n",
      "lemmatizing+filtering out nouns needs: 18.960478 seconds\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "path = \"/home/sim/all/Master/Forschungspraktikum/Tagesschau/transcripts/\"\n",
    "n_cpu_cores = 13 #int(os.cpu_count()-1)\n",
    "print(\"number cpu_cores to use:\",n_cpu_cores)\n",
    "\n",
    "#load s2t transcriptions\n",
    "path = \"/home/sim/all/Master/Forschungspraktikum/Tagesschau/transcripts/\"\n",
    "transcriptions = load_transcriptions(path)\n",
    "keys = ['2007','2008','2009','2010','2011','2012','2013','2014','2015','2016','2017','2018','2019','2020']\n",
    "years_dict = {}\n",
    "for year in keys: \n",
    "    years_dict[year] = []\n",
    "\n",
    "# lda for certain dates\n",
    "daterange = pd.date_range(datetime.datetime(2007, 1, 1), datetime.datetime(2020, 12, 31))\n",
    "dates_of_interest = []\n",
    "for date in daterange:\n",
    "    for i in transcriptions:\n",
    "        if 'tagesschau' in i and 'vor' not in i:\n",
    "            year = int((i.split('_')[1][4:8]))\n",
    "            if year==date.year:\n",
    "                month = int((i.split('_')[1][2:4]))\n",
    "                if month == date.month:\n",
    "                    day = int((i.split('_')[1][0:2]))\n",
    "                    if day == date.day:\n",
    "                        years_dict[str(year)].append(i)\n",
    "                        dates_of_interest.append(i)\n",
    "print(\"There are \",len(dates_of_interest),\"transcriptions in given daterange\")\n",
    "\n",
    "#for year in years_dict.keys():\n",
    "    #print(year)\n",
    "    #print(len(years_dict[str(year)]))\n",
    "\n",
    "#\"\"\"\n",
    "for year in years_dict.keys():\n",
    "    print(\"\\npreprocess transcriptions of year:\",year,\"quantity:\",len(years_dict[str(year)]))\n",
    "    if len(years_dict[str(year)]) == 0:\n",
    "        print(\"no transcription for year:\",year)\n",
    "        continue\n",
    "    start = datetime.datetime.now()\n",
    "    #transcriptions = transcriptions[i*50:(i+1)*50]\n",
    "    transcriptions = years_dict[str(year)]\n",
    "    # maybe: more threads -> more RAM so in case of low ram use less cores\n",
    "\n",
    "    manual_stopwords = ['wetter','ziel','zahl','stunden','weg','fernsehen','präsident'\n",
    "                        'damen','herren','menschen','land','abend','grad','nacht','euro','geld','regierung',\n",
    "                       'millionen','unternehmen','tagesthemen','angaben','präsident','himmelsrichtung',\n",
    "                       'milliarden','tagesschau','woche','wochen','leben']\n",
    "    df_processed,df1 = preprocess_transcriptions(transcriptions,number_cpu_cores=n_cpu_cores,manual_stopwords=manual_stopwords)\n",
    "    #print_workcloud(df_processed['preprocessed'])\n",
    "\n",
    "    # Initialise count vectorizer\n",
    "    count_vectorizer = CountVectorizer(ngram_range=(1,1))\n",
    "    # Fit and transform the processed titles\n",
    "    count_data = count_vectorizer.fit_transform(df_processed['preprocessed'])\n",
    "    #params for LDA\n",
    "    n_topics = 10 #alpha\n",
    "    n_words = 5 #beta\n",
    "\n",
    "    # Visualise the 10 most common words\n",
    "    #plot_25_most_common_words(count_data, count_vectorizer)\n",
    "    \n",
    "    startLDA = datetime.datetime.now()\n",
    "    \"\"\"\n",
    "    # with gensim lda\n",
    "    for i in range(len(df_processed['preprocessed'])):\n",
    "        df_processed.at[i,'preprocessed'] = df_processed.loc[i,'preprocessed'].split(\" \")\n",
    "    dictionary = corpora.Dictionary(df_processed['preprocessed'])\n",
    "    bow_corpus = [dictionary.doc2bow(doc) for doc in df_processed['preprocessed']]\n",
    "    lda_model = models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=20, workers=n_cpu_cores)\n",
    "    endLDA = datetime.datetime.now()\n",
    "    print(\"lda training needs:\",(endLDA-startLDA).total_seconds(),\"seconds\")\n",
    "    for idx, topic in lda_model.print_topics(-1):\n",
    "        print('Topic: {} \\nWords: {}'.format(idx+1, re.sub('[^A-Za-z ]+', '', topic).replace(\"  \",\" \")))\n",
    "        #print('Topic: {} \\nWords: {}'.format(idx,topic))\n",
    "    \n",
    "    #\"\"\"\n",
    "    # with sklearn lda\n",
    "    lda_model = train_lda(df_processed,count_vectorizer,count_data,number_topics=n_topics,number_words=n_words,ngram_tuple=(2,3),stop_words=None)\n",
    "    print_topics(lda_model, count_vectorizer, n_words)\n",
    "    del df_processed\n",
    "    del count_data\n",
    "    del lda_model\n",
    "    #print(\"loop needed:\",(end-start).total_seconds(),\"seconds\")\n",
    "#\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
